/**
 * Copyright 2013-2015 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten,
 *                     Benjamin Worpitz
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "types.h"
#include "particles/frame_types.hpp"

#include "simulation_defines.hpp"

#include "FieldJ.hpp"
#include "particles/memory/boxes/ParticlesBox.hpp"


#include "algorithms/Velocity.hpp"

#include "memory/boxes/CachedBox.hpp"
#include "dimensions/DataSpaceOperations.hpp"
#include "nvidia/functors/Add.hpp"
#include "mappings/threads/ThreadCollective.hpp"
#include "algorithms/Set.hpp"

#include "particles/frame_types.hpp"

namespace picongpu
{

using namespace PMacc;

typedef FieldJ::DataBoxType J_DataBox;

template<
    int workerMultiplier,
    typename BlockDescription_,
    uint32_t AREA>
struct KernelComputeCurrent
{
template<
    typename T_Acc,
    typename JBox,
    typename ParBox,
    typename FrameSolver,
    typename Mapping>
ALPAKA_FN_ACC void operator()(
    T_Acc const & acc,
    JBox const & fieldJ,
    ParBox const & boxPar,
    FrameSolver const & frameSolver,
    Mapping const & mapper) const
{
    static_assert(
        alpaka::dim::Dim<T_Acc>::value == simDim,
        "The KernelComputeCurrent functor has to be called with a simDim dimensional accelerator!");

    typedef typename ParBox::FrameType FrameType;
    typedef typename Mapping::SuperCellSize SuperCellSize;
    DataSpace<simDim> const blockIndex(alpaka::idx::getIdx<alpaka::Grid, alpaka::Blocks>(acc));
    DataSpace<simDim> const threadIndex(alpaka::idx::getIdx<alpaka::Block, alpaka::Threads>(acc));

    const uint32_t cellsPerSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;

    const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIndex)));

    /* thread id, can be greater than cellsPerSuperCell*/
    const int linearThreadIdx = DataSpaceOperations<simDim>::template map<SuperCellSize > (threadIndex);

    const uint32_t virtualBlockId = linearThreadIdx / cellsPerSuperCell;
    /* move linearThreadIdx for all threads to [0;cellsPerSuperCell) */
    const int virtualLinearId = linearThreadIdx - (virtualBlockId * cellsPerSuperCell);


    FrameType* frame = NULL;
    bool isValid = false;
    lcellId_t particlesInSuperCell = 0;

    frame = &(boxPar.getLastFrame(block, isValid));
    if (isValid && virtualBlockId == 0)
        particlesInSuperCell = boxPar.getSuperCell(block).getSizeLastFrame();

    /* select N-th (N=virtualBlockId) frame from the end of the list*/
    for (int i = 1; (i <= virtualBlockId) && isValid; ++i)
    {
        particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
        frame = &(boxPar.getPreviousFrame(*frame, isValid));
    }

    /* This memory is used by all virtual blocks*/
    auto cachedJ(CachedBox::create < 0, typename JBox::ValueType > (acc, BlockDescription_()));

    alpaka::block::sync::syncBlockThreads(acc);

    Set<typename JBox::ValueType > set(float3_X::create(0.0));
    ThreadCollective<BlockDescription_, cellsPerSuperCell * workerMultiplier> collectiveSet(linearThreadIdx);
    collectiveSet(set, cachedJ);

    alpaka::block::sync::syncBlockThreads(acc);

    while (isValid)
    {
        /* this test is only important for the last frame
         * if frame is not the last one particlesInSuperCell==particles count in supercell
         */
        if (virtualLinearId < particlesInSuperCell)
        {
            frameSolver(acc,
                        *frame,
                        virtualLinearId,
                        cachedJ);
        }

        particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
        for (int i = 0; (i < workerMultiplier) && isValid; ++i)
        {
            frame = &(boxPar.getPreviousFrame(*frame, isValid));
        }
    }

    /* we wait that all threads finish the loop*/
    alpaka::block::sync::syncBlockThreads(acc);

    nvidia::functors::Add add;
    const DataSpace<simDim> blockCell = block * SuperCellSize::toRT();
    ThreadCollective<BlockDescription_, cellsPerSuperCell * workerMultiplier> collectiveAdd(linearThreadIdx);
    PMACC_AUTO(fieldJBlock, fieldJ.shift(blockCell));
    collectiveAdd(add, fieldJBlock, cachedJ);
}
};

template<class ParticleAlgo, class Velocity, class TVec>
struct ComputeCurrentPerFrame
{

    HDINLINE ComputeCurrentPerFrame(const float_X deltaTime) :
    deltaTime(deltaTime)
    {
    }

    template<
        typename T_Acc,
        class FrameType,
        class BoxJ >
    DINLINE void operator()(
        T_Acc const & acc,
        FrameType& frame,
        const int localIdx,
        BoxJ & jBox) const
    {
        PMACC_AUTO(particle, frame[localIdx]);
        const float_X weighting = particle[weighting_];
        const floatD_X pos = particle[position_];
        const int particleCellIdx = particle[localCellIdx_];
        const float_X charge = attribute::getCharge(weighting,particle);
        const DataSpace<simDim> localCell(DataSpaceOperations<simDim>::template map<TVec > (particleCellIdx));

        Velocity velocity;
        const float3_X vel = velocity(
                                      particle[momentum_],
                                      attribute::getMass(weighting,particle));
        PMACC_AUTO(fieldJShiftToParticle, jBox.shift(localCell));
        ParticleAlgo perParticle;
        perParticle(acc,
                    fieldJShiftToParticle,
                    pos,
                    vel,
                    charge,
                    deltaTime
                    );
    }

private:
    PMACC_ALIGN(deltaTime, const float);
};

struct KernelAddCurrentToEMF
{
template<
    typename T_Acc,
    typename T_CurrentInterpolation,
    typename Mapping>
ALPAKA_FN_ACC void operator()(
    T_Acc const & acc,
    FieldE::DataBoxType const & fieldE,
    FieldB::DataBoxType const & fieldB,
    J_DataBox const & fieldJ,
    T_CurrentInterpolation const & currentInterpolation,
    Mapping const & mapper) const
{
    static_assert(
        alpaka::dim::Dim<T_Acc>::value == simDim,
        "The KernelIntensity functor has to be called with a simDim dimensional accelerator!");

    /* Caching of fieldJ */
    typedef SuperCellDescription<
                SuperCellSize,
                typename T_CurrentInterpolation::LowerMargin,
                typename T_CurrentInterpolation::UpperMargin
                > BlockArea;

    DataSpace<simDim> const blockIndex(alpaka::idx::getIdx<alpaka::Grid, alpaka::Blocks>(acc));
    DataSpace<simDim> const threadIndex(alpaka::idx::getIdx<alpaka::Block, alpaka::Threads>(acc));

    auto cachedJ(CachedBox::create < 0, typename J_DataBox::ValueType > (acc, BlockArea()));

    nvidia::functors::Assign assign;
    const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIndex)));
    const DataSpace<simDim> blockCell = block * MappingDesc::SuperCellSize::toRT();

    PMACC_AUTO(fieldJBlock, fieldJ.shift(blockCell));

    ThreadCollective<BlockArea> collective(threadIndex);
    collective(
              assign,
              cachedJ,
              fieldJBlock
              );

    alpaka::block::sync::syncBlockThreads(acc);

    const DataSpace<Mapping::Dim> cell(blockCell + threadIndex);

    // Amperes Law:
    //   Change of the dE = - j / EPS0 * dt
    //                        j = current density (= current per area)
    //                          = fieldJ
    currentInterpolation( fieldE.shift(cell), fieldB.shift(cell), cachedJ.shift(threadIndex) );
}
};

struct KernelBashCurrent
{
template<
    typename T_Acc,
    typename Mapping>
ALPAKA_FN_ACC void operator()(
    T_Acc const & acc,
    J_DataBox const & fieldJ,
    J_DataBox const & targetJ,
    DataSpace<simDim> const & exchangeSize,
    DataSpace<simDim> const & direction,
    Mapping const & mapper) const
{
    static_assert(
        alpaka::dim::Dim<T_Acc>::value == simDim,
        "The KernelBashCurrent functor has to be called with a simDim dimensional accelerator!");

    DataSpace<simDim> const blockIndex(alpaka::idx::getIdx<alpaka::Grid, alpaka::Blocks>(acc));
    DataSpace<simDim> const threadIndex(alpaka::idx::getIdx<alpaka::Block, alpaka::Threads>(acc));

    const DataSpace<simDim> blockCell(
                                      mapper.getSuperCellIndex(DataSpace<simDim > (blockIndex))
                                      * Mapping::SuperCellSize::toRT()
                                      );
    const DataSpace<Mapping::Dim> sourceCell(blockCell + threadIndex);

    /*origin in area from local GPU*/
    DataSpace<simDim> nullSourceCell(
                                     mapper.getSuperCellIndex(DataSpace<simDim > ())
                                     * Mapping::SuperCellSize::toRT()
                                     );
    DataSpace<simDim> targetCell(sourceCell - nullSourceCell);

    for (uint32_t d = 0; d < simDim; ++d)
    {
        if (direction[d] == -1)
        {
            if (threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d]) return;
            targetCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
        }
        else if ((direction[d] == 1) && (threadIndex[d] >= exchangeSize[d])) return;
    }

    targetJ(targetCell) = fieldJ(sourceCell);
}
};

struct KernelInsertCurrent
{
template<
    typename T_Acc,
    typename Mapping>
ALPAKA_FN_ACC void operator()(
    T_Acc const & acc,
    J_DataBox const & fieldJ,
    J_DataBox const & sourceJ,
    DataSpace<simDim> const & exchangeSize,
    DataSpace<simDim> const & direction,
    Mapping const & mapper) const
{
    static_assert(
        alpaka::dim::Dim<T_Acc>::value == simDim,
        "The KernelInsertCurrent functor has to be called with a simDim dimensional accelerator!");

    DataSpace<simDim> const blockIndex(alpaka::idx::getIdx<alpaka::Grid, alpaka::Blocks>(acc));
    DataSpace<simDim> const threadIndex(alpaka::idx::getIdx<alpaka::Block, alpaka::Threads>(acc));

    const DataSpace<simDim> blockCell(
                                      mapper.getSuperCellIndex(DataSpace<simDim > (blockIndex))
                                      * Mapping::SuperCellSize::toRT()
                                      );
    DataSpace<Mapping::Dim> targetCell(blockCell + threadIndex);

    /*origin in area from local GPU*/
    DataSpace<simDim> nullSourceCell(
                                     mapper.getSuperCellIndex(DataSpace<simDim > ())
                                     * Mapping::SuperCellSize::toRT()
                                     );
    DataSpace<simDim> sourceCell(targetCell - nullSourceCell);

    for (uint32_t d = 0; d < simDim; ++d)
    {
        if (direction[d] == 1)
        {
            if (threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d]) return;
            sourceCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
            targetCell[d] -= SuperCellSize::toRT()[d];
        }
        else if (direction[d] == -1)
        {
            if (threadIndex[d] >= exchangeSize[d]) return;
            targetCell[d] += SuperCellSize::toRT()[d];
        }
    }

    fieldJ(targetCell) += sourceJ(sourceCell);
}
};

}
